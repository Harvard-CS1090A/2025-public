{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bb71e8",
   "metadata": {
    "id": "bootstrap-intro"
   },
   "source": [
    "# CS1090a Introduction to Data Science\n",
    "\n",
    "## Section 5: Bootstrap and Inference for Regression\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2025**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Kevin Rader<br/>\n",
    "**Preceptor**: Chris Gumb\n",
    "<hr style='height:2px'>\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In previous sections, we focused on building models to make predictions. But how confident are we in those predictions? And how much would our model change if we had a slightly different dataset?\n",
    "\n",
    "**Bootstrapping** is a powerful statistical method that helps us answer these questions. It's a resampling technique that allows us to estimate the uncertainty of any statistic (like a regression coefficient) by simulating the process of drawing multiple datasets from our original data.\n",
    "\n",
    "In this section, we will:\n",
    "1. Use bootstrapping to estimate the uncertainty of a simple linear regression model's coefficients.\n",
    "2. Visualize this uncertainty by plotting multiple bootstrapped regression lines.\n",
    "3. Calculate a 95% confidence interval (CI) for the slope of our regression line.\n",
    "4. Calculate a 95% CI for a specific prediction.\n",
    "5. Compare our bootstrapped CIs to the analytical CIs provided by statistical packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e6100",
   "metadata": {
    "id": "bootstrap-imports"
   },
   "outputs": [],
   "source": [
    "# Data and Stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import t\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Intelligence packages\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de10a3",
   "metadata": {
    "id": "bootstrap-colab-setup"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Create the data directory if it doesn't exist\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    # Download the data file if it doesn't exist\n",
    "    if not os.path.exists('data/camberville_housing.csv'):\n",
    "        !curl -L \"https://github.com/Harvard-CS1090A/2025-public/raw/refs/heads/main/sec05/data/camberville_housing.csv\" -o \"data/camberville_housing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3d13d",
   "metadata": {},
   "source": [
    "### 1. Load and Visualize the Data\n",
    "\n",
    "We'll use a dataset of housing sales in the Cambridge/Somerville area. Our goal is to model the relationship between the living area (in square feet) and the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f5dc5",
   "metadata": {
    "id": "bootstrap-load-data"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/camberville_housing.csv\")\n",
    "\n",
    "# For simplicity, let's focus on houses with reasonable living area and price\n",
    "df = df[(df['sqft'] > 500) & (df['sqft'] < 5000)]\n",
    "df = df[(df['price'] > 100_000) & (df['price'] < 4_000_000)]\n",
    "\n",
    "# Define our predictor (X) and response (y)\n",
    "X = df[['sqft']]\n",
    "y = df['price']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d253f",
   "metadata": {},
   "source": [
    "First, let's visualize the relationship between living area and sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff7cf3",
   "metadata": {
    "id": "bootstrap-eda-plot"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='sqft', y='price', data=df, alpha=0.6)\n",
    "plt.title('Housing Price vs. Living Area in Harvard Square')\n",
    "plt.xlabel('Living Area (sq. ft.)')\n",
    "plt.ylabel('Sale Price ($)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaeb01",
   "metadata": {},
   "source": [
    "The scatter plot shows a positive linear relationship, but also reveals non-constant variance (heteroscedasticity)â€”the variability of sale price increases as living area increases. This is a perfect scenario to use bootstrap, as it doesn't rely on the standard assumptions of linear regression (like constant variance).\n",
    "\n",
    "### 2. Bootstrap the Regression Model\n",
    "\n",
    "Now, let's perform the bootstrap procedure. We will repeat the following steps 1,000 times:\n",
    "1.  **Resample:** Create a new, \"bootstrapped\" dataset by sampling **with replacement** from our original data. This new dataset will have the same size as the original.\n",
    "2.  **Fit:** Fit a simple linear regression model on the bootstrapped dataset.\n",
    "3.  **Store:** Save the slope (`coef_`) and intercept (`intercept_`) of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82cb4c",
   "metadata": {
    "id": "bootstrap-loop"
   },
   "outputs": [],
   "source": [
    "B = 1000\n",
    "boot_slopes = []\n",
    "boot_intercepts = []\n",
    "\n",
    "for i in range(B):\n",
    "    # Create a bootstrap sample from the dataframe\n",
    "    df_boot = ...\n",
    "    \n",
    "    # Define the predictor and response for the bootstrap sample\n",
    "    X_boot = ...\n",
    "    y_boot = ...\n",
    "    \n",
    "    # Fit a linear regression model\n",
    "    model_boot = ...\n",
    "    \n",
    "    # Store the slope and intercept\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "print(f\"Completed {B} bootstrap iterations.\")\n",
    "print(f\"First 5 slopes: {boot_slopes[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeda0e",
   "metadata": {},
   "source": [
    "### 3. Visualize the Bootstrapped Models\n",
    "\n",
    "By fitting a model on 1,000 different (but related) datasets, we have effectively simulated 1,000 possible regression lines that our data could have produced. Plotting them all together gives us a visual sense of the model's uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337089e5",
   "metadata": {
    "id": "bootstrap-plot-lines"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the original data\n",
    "sns.scatterplot(x='sqft', y='price', data=df, alpha=0.2, label='Original Data')\n",
    "\n",
    "# Plot the bootstrapped regression lines\n",
    "x_range = np.array([X.min(), X.max()])\n",
    "\n",
    "# Plot one line with a label for the legend\n",
    "y_range_example = boot_intercepts[0] + boot_slopes[0] * x_range\n",
    "plt.plot(x_range, y_range_example, color='cornflowerblue', alpha=0.1, label='Bootstrapped Models')\n",
    "for i in range(1, B):\n",
    "    y_range = boot_intercepts[i] + boot_slopes[i] * x_range\n",
    "    plt.plot(x_range, y_range, color='cornflowerblue', alpha=0.05)\n",
    "\n",
    "# Fit and plot the original model line\n",
    "original_model = LinearRegression().fit(X, y)\n",
    "original_y_range = original_model.intercept_ + original_model.coef_[0] * x_range\n",
    "plt.plot(x_range, original_y_range, color='red', lw=2, linestyle='--', label='Original Model')\n",
    "\n",
    "plt.title('Bootstrapped Regression Lines')\n",
    "plt.xlabel('Living Area (sq. ft.)')\n",
    "plt.ylabel('Sale Price ($)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dd6ce",
   "metadata": {},
   "source": [
    "The fan-like shape of the grey lines clearly shows that the model is more uncertain for predictions at the extreme ends of the living area range. Notice also the slight curvature in the data, especially for smaller houses; the straight regression lines seem to systematically overestimate prices for the very smallest houses and underestimate them for slightly larger ones.\n",
    "\n",
    "### 4. Analyze the Slope's Uncertainty\n",
    "\n",
    "We now have a distribution of 1,000 possible slopes. We can analyze this distribution to quantify our uncertainty about the true relationship between living area and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b80282",
   "metadata": {
    "id": "bootstrap-slope-hist"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(boot_slopes, bins=30, kde=True)\n",
    "plt.title('Distribution of Bootstrapped Slopes')\n",
    "plt.xlabel('Slope (Price per sq. ft.)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c661fde",
   "metadata": {},
   "source": [
    "From this distribution, we can calculate a **95% confidence interval** for the slope. This is the range within which we are 95% confident the \"true\" slope lies. We can find it by taking the 2.5th and 97.5th percentiles of our bootstrapped slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073d1c5",
   "metadata": {
    "id": "bootstrap-slope-ci"
   },
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval for the slope\n",
    "slope_ci = ...\n",
    "\n",
    "print(f\"The mean bootstrapped slope is: {np.mean(boot_slopes):.2f}\")\n",
    "print(f\"95% Confidence Interval for the slope: [{slope_ci[0]:.2f}, {slope_ci[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff26e6",
   "metadata": {},
   "source": [
    "### 5. Confidence Interval for a Prediction\n",
    "\n",
    "We can also use our bootstrapped models to find a confidence interval for a specific prediction. This is the CI for the *average* price at a given living area, not the range for a single house (which is a prediction interval).\n",
    "\n",
    "Let's find the 95% CI for the average price of a house with 2,000 sq. ft. of living area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a9a44",
   "metadata": {
    "id": "bootstrap-prediction-ci"
   },
   "outputs": [],
   "source": [
    "x_value = 2000\n",
    "predictions_at_x = []\n",
    "\n",
    "# For each bootstrapped model, calculate the prediction at x_value\n",
    "for i in range(B):\n",
    "    pred = ...\n",
    "    predictions_at_x.append(pred)\n",
    "",
    "# Calculate the 95% confidence interval for the prediction\n",
    "prediction_ci = ...\n",
    "\n",
    "print(f\"95% CI for the average price at {x_value} sq. ft.: [${prediction_ci[0]:,.2f}, ${prediction_ci[1]:,.2f}]\")\n",
    "\n",
    "# Plot the distribution of predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(predictions_at_x, bins=30, kde=True)\n",
    "plt.title(f'Distribution of Predicted Prices at {x_value} sq. ft.')\n",
    "plt.xlabel('Predicted House Value ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(prediction_ci[0], color='r', linestyle='--', label='95% CI')\n",
    "plt.axvline(prediction_ci[1], color='r', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f2cf1",
   "metadata": {},
   "source": [
    "### 6. Comparison with Analytical Formulas\n",
    "\n",
    "While `scikit-learn` is excellent for building predictive models, it is not primarily focused on statistical inference. For tasks like calculating confidence intervals, p-values, and other statistical measures, the `statsmodels` library is the standard tool in Python. It provides a rich set of results that are crucial for interpreting the uncertainty of a model's parameters. We use it here to easily get the \"textbook\" analytical confidence intervals to compare with our bootstrapped results.\n",
    "\n",
    "---\n",
    "\n",
    "For simple linear regression, there are mathematical formulas to calculate these confidence intervals. We can use the `statsmodels` library to compute them and compare them to our bootstrapped results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee0a46",
   "metadata": {
    "id": "bootstrap-statsmodels"
   },
   "outputs": [],
   "source": [
    "# Fit an OLS model using statsmodels\n",
    "model_sm = smf.ols('price ~ sqft', data=df).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dbc16",
   "metadata": {},
   "source": [
    "In the summary table above, look at the row for `sqft`. The columns `[0.025` and `0.975]` give the analytical 95% CI for the slope.\n",
    "\n",
    "Now, let's get the analytical CI for the prediction at 2,000 sq. ft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bb3f3",
   "metadata": {
    "id": "bootstrap-statsmodels-pred"
   },
   "outputs": [],
   "source": [
    "# Get the prediction summary frame from statsmodels\n",
    "pred_summary = model_sm.get_prediction(pd.DataFrame({'sqft': [x_value]})).summary_frame()\n",
    "display(pred_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb594055",
   "metadata": {},
   "source": [
    "The `mean_ci_lower` and `mean_ci_upper` columns give the analytical 95% CI for the mean response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4719997",
   "metadata": {
    "id": "bootstrap-comparison-table"
   },
   "outputs": [],
   "source": [
    "# Extract analytical CIs\n",
    "analytical_slope_ci = model_sm.conf_int().loc['sqft']\n",
    "analytical_pred_ci = pred_summary[['mean_ci_lower', 'mean_ci_upper']].iloc[0]\n",
    "\n",
    "# Create data for the comparison table\n",
    "comparison_data = {\n",
    "    \"Bootstrap CI\": [\n",
    "        f\"[{slope_ci[0]:.2f}, {slope_ci[1]:.2f}]\",\n",
    "        f\"[${prediction_ci[0]/1e6:.3f}M, ${prediction_ci[1]/1e6:.3f}M]\"\n",
    "    ],\n",
    "    \"Analytical CI (statsmodels)\": [\n",
    "        f\"[{analytical_slope_ci.iloc[0]:.2f}, {analytical_slope_ci.iloc[1]:.2f}]\",\n",
    "        f\"[${analytical_pred_ci.iloc[0]/1e6:.3f}M, ${analytical_pred_ci.iloc[1]/1e6:.3f}M]\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create and display the DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data, index=['Slope', f'Prediction at {x_value} sqft'])\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c62e5",
   "metadata": {},
   "source": [
    "The results are very close! Notice that the bootstrapped confidence intervals are slightly wider than the analytical ones. This is common, as the bootstrap makes fewer assumptions (like normality of errors) which the analytical formulas rely on. This demonstrates that bootstrapping provides a reliable way to estimate uncertainty, especially when the assumptions of classical methods are not fully met.\n",
    "\n",
    "#### A Deeper Dive: The Formulas Behind the CIs (Optional)\n",
    "\n",
    "The analytical 95% confidence interval for the slope ($\\beta_1$) is calculated as:\n",
    "$$ \\hat{\\beta}_1 \\pm t_{n-2, 0.975} \\cdot SE(\\hat{\\beta}_1) $$\n",
    "where $SE(\\hat{\\beta}_1)$ is the standard error of the slope coefficient:\n",
    "$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}} \\quad \\text{and} \\quad \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-2} $$\n",
    "Here, RSS is the Residual Sum of Squares. The term $t_{n-2, 0.975}$ is the critical value from a t-distribution with $n-2$ degrees of freedom that corresponds to a 95% confidence level (i.e., leaving 2.5% in the upper tail, so we look up the 97.5th percentile).\n",
    "\n",
    "The 95% confidence interval for the mean prediction at a new point $x_0$ is:\n",
    "$$ \\hat{y}_0 \\pm t_{n-2, 0.975} \\cdot SE(\\hat{y}_0) $$\n",
    "where $SE(\\hat{y}_0)$ is the standard error of the mean prediction:\n",
    "$$ SE(\\hat{y}_0) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}} $$\n",
    "\n",
    "Here are the NumPy calculations for these formulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bd388",
   "metadata": {
    "id": "bootstrap-numpy-ci"
   },
   "outputs": [],
   "source": [
    "# Fit the original model again to ensure we have the coefficients\n",
    "original_model = LinearRegression().fit(X, y)\n",
    "y_pred = original_model.predict(X)\n",
    "\n",
    "# Model parameters\n",
    "n = len(df)\n",
    "beta_1_hat = original_model.coef_[0]\n",
    "beta_0_hat = original_model.intercept_\n",
    "\n",
    "# Calculate RSS and sigma_hat\n",
    "rss = np.sum((y - y_pred)**2)\n",
    "sigma_hat = np.sqrt(rss / (n - 2))\n",
    "\n",
    "# Denominator term for SE formulas\n",
    "ssx = np.sum((X['sqft'] - X['sqft'].mean())**2)\n",
    "\n",
    "# --- CI for the Slope ---\n",
    "se_beta_1 = np.sqrt(sigma_hat**2 / ssx)\n",
    "t_crit = t.ppf(0.975, df=n-2)\n",
    "slope_ci_numpy = [beta_1_hat - t_crit * se_beta_1, beta_1_hat + t_crit * se_beta_1]\n",
    "\n",
    "print(\"--- Slope CI (NumPy) ---\")\n",
    "print(f\"SE(beta_1): {se_beta_1:.4f}\")\n",
    "print(f\"t-critical value: {t_crit:.4f}\")\n",
    "print(f\"95% CI for slope: [{slope_ci_numpy[0]:.2f}, {slope_ci_numpy[1]:.2f}]\")\n",
    "\n",
    "# --- CI for the Prediction ---\n",
    "x_0 = 2000.0\n",
    "y_0_hat = beta_0_hat + beta_1_hat * x_0\n",
    "se_y_0 = sigma_hat * np.sqrt(1/n + (x_0 - X['sqft'].mean())**2 / ssx)\n",
    "pred_ci_numpy = [y_0_hat - t_crit * se_y_0, y_0_hat + t_crit * se_y_0]\n",
    "\n",
    "print(\"\\n--- Prediction CI (NumPy) ---\")\n",
    "print(f\"SE(y_hat_0): {se_y_0:.2f}\")\n",
    "print(f\"95% CI for prediction at x={x_0:.0f}: [${pred_ci_numpy[0]:,.2f}, ${pred_ci_numpy[1]:,.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0669b5",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Bootstrapping is an incredibly versatile tool. We used it here for simple linear regression, but the same principle can be applied to almost any model or statistic. By resampling our data, we can simulate the variability we'd expect to see across different datasets, giving us a robust way to quantify the uncertainty in our models and predictions.\n",
    "\n",
    "#### A Final Thought: How Could We Improve This Model?\n",
    "\n",
    "Our analysis revealed a potential weakness in our simple linear model: a systematic bias at lower `sqft` values, suggesting the relationship isn't perfectly linear. \n",
    "\n",
    "Given what you know about regression, what techniques could you use to capture this curvature and potentially create a better-fitting model? Bootstrapping would remain a valuable tool for assessing the uncertainty of these more complex models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
